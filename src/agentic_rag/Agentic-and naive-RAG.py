# -*- coding: utf-8 -*-
"""myRAG_(2) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GWIFIFP2DOt0BBzbLGkAk-MW12-3TFm9

# RAG Implementation On FiQA Dataset

# __Summary__
__Source Dataset:__ BeIR/fiqa-qrels,  BeIR/fiqa

__Evaluation Metrics:__

## Environment Setup
"""

!pip install ragas
!pip install nltk
!pip install langfuse openai
!pip install transformers accelerate einops langchain xformers bitsandbytes sentence_transformers chromadb
!pip install langchain-community langchain-core
!pip install kagglehub
!pip install datasets
!pip install langchain-openai
!pip install -U bitsandbytes
!pip install anyio==3.7.1
!pip install pytrec_eval

import os
os.environ["OPENAI_API_KEY"] = 'your-key'

from copy import copy
from datasets import load_dataset, concatenate_datasets
import kagglehub
from langchain.llms import HuggingFacePipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain_openai.chat_models import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain.document_loaders import DirectoryLoader
from langchain.schema import Document
from langfuse import Langfuse
import numpy as np
import ragas
from ragas.metrics import (Faithfulness, ResponseRelevancy, LLMContextPrecisionWithoutReference)
from ragas.run_config import RunConfig
from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas import evaluate
from time import time
import torch
from torch import cuda, bfloat16
import transformers
from transformers import AutoTokenizer
import pytrec_eval

torch.cuda.is_available()

from huggingface_hub import login

hf_token = "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"
login(token=hf_token)

"""## RAG Setup"""

model_id = "Qwen/Qwen2.5-3B-Instruct"

device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16
)

time_1 = time()
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
time_2 = time()
print(f"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.")

time_1 = time()
query_pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        device_map="auto",
        max_new_tokens=1000,
        truncation=True)
time_2 = time()
print(f"Prepare pipeline: {round(time_2-time_1, 3)} sec.")

llm = HuggingFacePipeline(pipeline=query_pipeline)
llm(prompt="Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.")

model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": "cuda"}

embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)

"""## Dataset"""

from mlcroissant import Dataset

ds = Dataset(jsonld="https://huggingface.co/api/datasets/BeIR/fiqa/croissant")
ds

bier_fiqa_documents = load_dataset("BeIR/fiqa", "corpus")
bier_fiqa_queries = load_dataset("BeIR/fiqa", "queries")
bier_fiqa_qrels = load_dataset("BeIR/fiqa-qrels")
explodinggradients_fiqa_eval = load_dataset("explodinggradients/fiqa", "ragas_eval")['baseline']
#ds = load_dataset("LLukas22/fiqa")

queries_ds = bier_fiqa_queries['queries']
docs_ds = bier_fiqa_documents['corpus']
fiqa_scores = concatenate_datasets([bier_fiqa_qrels['train'], bier_fiqa_qrels['validation'], bier_fiqa_qrels['test']])
fiqa_scores

qry_doc_dict = {}
data_limit = 5
for i in range(min([data_limit, len(fiqa_scores)])):
    qry_id = str(fiqa_scores['query-id'][i])
    doc_id = str(fiqa_scores['corpus-id'][i])
    try:
        qry = queries_ds[queries_ds['_id'].index(qry_id)]['text']
        doc = docs_ds[docs_ds['_id'].index(doc_id)]['text']
        if qry not in qry_doc_dict:
          qry_doc_dict[qry] = ''
        qry_doc_dict[qry] += doc + '\n'

    except:
        pass

query_count = len(set(qry_doc_dict))
fiqa_scores = fiqa_scores.select(range(query_count))
fiqa_scores = fiqa_scores.add_column('question', list(set(qry_doc_dict.keys())))
fiqa_scores = fiqa_scores.add_column('reference', list(qry_doc_dict.values()))
fiqa_scores = fiqa_scores.remove_columns(['query-id', 'corpus-id', 'score'])
fiqa_scores

documents = []
counter = 0
for doc in fiqa_scores['reference']:
    doc = Document(
        page_content=doc,
        metadata={'source': str(counter)})
    documents.append(doc)
    counter += 1

"""## RAG Initialize"""

all_splits = text_splitter.split_documents(documents)
vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory="chroma_db")
retriever = vectordb.as_retriever()

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents = True,
    verbose=True
)

results = []
contexts = []
prompts = []
limiting_prompt = "" #" Keep it under 250 words."
prompt_stopping_text = "hel"
answer_staring_text = "Helpful Answer: "
#answer_stopping_text = " Question"

for i in range(len(fiqa_scores)):
    output = qa(fiqa_scores[i]['question'] + limiting_prompt)
    start_idx = output['result'].find(answer_staring_text)
    #stop_idx = output['result'][idx + len(answer_staring_text):].find(answer_stopping_text)
    results.append(output['result'][start_idx + len(answer_staring_text):])# stop_idx])
    prompts.append(output['result'][:start_idx])
    contexts.append(output['source_documents'])

print('Sample RAG chain output:')
print('question:', fiqa_scores[0]['question'])
print('------------------------------------')
print('prompt:', prompts[0])
print('------------------------------------')
print('Answer:', results[0])
print('------------------------------------')

context_processed = []

for documents in contexts:
    doc_list = [d.page_content for d in documents]
    context_processed.append(doc_list)

fiqa_scores = fiqa_scores.add_column('answer', results)
fiqa_scores = fiqa_scores.add_column('contexts', context_processed)
fiqa_scores

"""## RAG Evaluation"""

fiqa_scores

result = evaluate(fiqa_scores)
print(result)

"""### Evaluation Using Pytrec"""

## finding relevent docs for each query

qrels = {
    "query_id_1": {"doc1": 1, "doc2": 1},  # relevant documents
    "query_id_2": {"doc3": 1}
}



qrel = {}
for i in range(len(fiqa_scores)):
    query_id = str(fiqa_scores[i]['query-id'])
    corpus_id = str(fiqa_scores[i]['corpus-id'])
    score = fiqa_scores[i]['score']

    if query_id not in qrel:
        qrel[query_id] = {}
    qrel[query_id][corpus_id] = score

evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map', 'ndcg'})


run = {}

for i in range(len(fiqa_scores)):
  query_id = str(fiqa_scores[i]['query-id'])
  corpus_id = str(fiqa_scores[i]['corpus-id'])
  score = fiqa_scores[i]['score']
  if query_id not in run:
      run[query_id] = {}
  run[query_id][corpus_id] = score

results = evaluator.evaluate(run)


for query_id, query_measures in sorted(results.items()):
    print(query_id, query_measures)

map_scores = [query_measures['map'] for query_measures in results.values()]
ndcg_scores = [query_measures['ndcg'] for query_measures in results.values()]
print(f"Average MAP: {sum(map_scores) / len(map_scores)}")
print(f"Average NDCG: {sum(ndcg_scores) / len(ndcg_scores)}")





metrics = [
    Faithfulness(),
    ResponseRelevancy(),
    LLMContextPrecisionWithoutReference(),
]

def init_ragas_metrics(metrics, llm, embedding):
    for metric in metrics:
        if isinstance(metric, MetricWithLLM):
            metric.llm = llm
        if isinstance(metric, MetricWithEmbeddings):
            metric.embeddings = embedding
        run_config = RunConfig()
        metric.init(run_config)

init_ragas_metrics(
    metrics,
    llm=LangchainLLMWrapper(llm),
    embedding=LangchainEmbeddingsWrapper(embeddings),
)

row = explodinggradients_fiqa_eval[0]
row['question'], row['answer']



from ragas.dataset_schema import SingleTurnSample

async def score_with_ragas(query, chunks, answer):
    scores = {}
    for m in metrics:
        sample = SingleTurnSample(
            user_input=query,
            retrieved_contexts=chunks,
            response=answer,
        )
        print(f"calculating {m.name}")
        scores[m.name] = await m.single_turn_ascore(sample)
    return scores

# start a new trace when you get a question
question = row['question']
trace = langfuse.trace(name = "rag")

contexts = row['contexts']


trace.span(
    name = "retrieval", input={'question': question}, output={'contexts': contexts}
)

answer = row['answer']
trace.span(
    name = "generation", input={'question': question, 'contexts': contexts}, output={'answer': answer}
)

# send the scores
for m in metrics:
    trace.score(name=m.name, value=ragas_scores[m.name])

explodinggradients_fiqa_eval

explodinggradients_fiqa_eval = explodinggradients_fiqa_eval.rename_column("ground_truths", "reference")
explodinggradients_fiqa_eval

ds = explodinggradients_fiqa_eval.map(lambda x: {'reference': x['reference'][0] if isinstance(x['reference'], list) and len(x['reference']) > 0 else ''})

async def score_with_ragas(query, chunks, answer):
    scores = {}
    for m in metrics:
        sample = SingleTurnSample(
            user_input=query,
            retrieved_contexts=chunks,
            response=answer,
        )
        print(f"calculating {m.name}")
        scores[m.name] = await m.single_turn_ascore(sample)
    return scores

await score_with_ragas(row['question'], row['contexts'], row['answer'])
